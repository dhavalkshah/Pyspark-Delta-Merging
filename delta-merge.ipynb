%python
from pyspark.sql.window import Window
from pyspark.sql.functions import col, row_number
from pyspark.sql.functions import lit, split
from delta.tables import *



columns = ["user_id","customer_id","account_id","premise_id","created_by","event_name","event_value","event_time"]
# data = [("123","1","2","3","me","sms","766","10 AM")]
data  = [("1_1_1","1","1","1","a","SMS","1","1"), \
("1_1_1","1","1","1","a","SMS","1","2"), \
("1_1_1","1","1","1","b","MOBILE","M","1"), \
("1_1_1","1","1","1","b","MOBILE","M","2"), \
("2_2_2","2","2","2","a","SMS","1","1"), \
("2_2_2","2","2","2","a","SMS","1","2"), \
("2_2_2","2","2","2","b","MOBILE","M","1"), \
("2_2_2","2","2","2","b","MOBILE","M","2")]

df = spark.createDataFrame(data,columns)

w2 = Window.partitionBy("user_id", "event_name").orderBy(col("event_time").desc())
df = df.withColumn("row",row_number().over(w2)) \
  .filter(col("row") == 1).drop("row")
print("Initial DF")
df.show()

print("Delta Table1")
spark.sql("select * from ap1").show();

deltaTablePref = DeltaTable.forName(spark, "ap1")

deltaTablePref.alias("ap").merge(
    source = df.alias("updates"),
    condition = "ap.user_id = updates.user_id AND ap.preference_type=updates.event_name"
  ).whenMatchedUpdate(set =
    {
      "ap.preference_value": "updates.event_value",
      "ap.preference_type": "updates.event_name",
      "ap.updated_at": "updates.event_time"
    }
  ).whenNotMatchedInsert(values =
    {
      "ap.user_id": "updates.user_id",
      "ap.customer_id": "updates.customer_id",
      "ap.account_id": "updates.account_id",
      "ap.premise_id": "updates.premise_id",
      "ap.created_by": "updates.created_by",
      "ap.preference_value": "updates.event_value",
      "ap.preference_type": "updates.event_name",
      "ap.created_at": "updates.event_time",
      "ap.updated_at": "updates.event_time"
    }
  ).execute()

# deltaTablePref.alias('ap') \
#   .merge( df.alias('updates'), \
#          ('ap.user_id = updates.user_id') and ('ap.preference_type=updates.event_name')) \
#   .whenMatchedUpdate(set = { 
#     "ap.preference_value": "updates.event_value",
#     "ap.preference_type": "updates.event_name",
#     "ap.updated_at": "updates.event_time"
#   }) \
#   .whenNotMatchedInsert(values = {
#     "ap.user_id": "updates.user_id",
#     "ap.customer_id": "updates.customer_id",
#     "ap.account_id": "updates.account_id",
#     "ap.premise_id": "updates.premise_id",
#     "ap.created_by": "updates.created_by",
#     "ap.preference_value": "updates.event_value",
#     "ap.preference_type": "updates.event_name",
#     "ap.created_at": "updates.event_time",
#     "ap.updated_at": "updates.event_time"     
#   }) \
#   .execute()

print("Delta Table2")
spark.sql("select * from ap1").show();

updData = [("1_1_1","1","1","1","a","SMS","1","1"), \
("1_1_1","1","1","1","a","SMS","2","2"), \
("1_1_1","1","1","1","b","MOBILE","M1","1"), \
("1_1_1","1","1","1","b","MOBILE","M2","2"), \
("2_2_2","2","2","2","a","SMS","3","1"), \
("2_2_2","2","2","2","a","SMS","4","2"), \
("2_2_2","2","2","2","b","MOBILE","M3","1"), \
("2_2_2","2","2","2","b","MOBILE","M4","2")]

updDf = spark.createDataFrame(updData,columns)
w2 = Window.partitionBy("user_id", "event_name").orderBy(col("event_time").desc())
updDf = updDf.withColumn("row",row_number().over(w2)) \
  .filter(col("row") == 1).drop("row")

print("Updated Data")
updDf.show()

deltaTablePref.alias("ap").merge(
    source = updDf.alias("updates"),
    condition = "ap.user_id = updates.user_id AND ap.preference_type=updates.event_name"
  ).whenMatchedUpdate(set =
    {
      "ap.preference_value": "updates.event_value",
      "ap.preference_type": "updates.event_name",
      "ap.updated_at": "updates.event_time"
    }
  ).whenNotMatchedInsert(values =
    {
      "ap.user_id": "updates.user_id",
      "ap.customer_id": "updates.customer_id",
      "ap.account_id": "updates.account_id",
      "ap.premise_id": "updates.premise_id",
      "ap.created_by": "updates.created_by",
      "ap.preference_value": "updates.event_value",
      "ap.preference_type": "updates.event_name",
      "ap.created_at": "updates.event_time",
      "ap.updated_at": "updates.event_time"
    }
  ).execute()

# deltaTablePref.alias('ap') \
#   .merge( updDf.alias('updates'), \
#          ('ap.user_id = updates.user_id') and ('ap.preference_type=updates.event_name')) \
#   .whenMatchedUpdate(set = { 
#     "ap.preference_value": "updates.event_value",
#     "ap.preference_type": "updates.event_name",
#     "ap.updated_at": "updates.event_time"
#   }) \
#   .whenNotMatchedInsert(values = {
#     "ap.user_id": "updates.user_id",
#     "ap.customer_id": "updates.customer_id",
#     "ap.account_id": "updates.account_id",
#     "ap.premise_id": "updates.premise_id",
#     "ap.created_by": "updates.created_by",
#     "ap.preference_value": "updates.event_value",
#     "ap.preference_type": "updates.event_name",
#     "ap.created_at": "updates.event_time",
#     "ap.updated_at": "updates.event_time"     
#   }) \
#   .execute()

print("Deltat table 3")
spark.sql("select * from ap1").show();
